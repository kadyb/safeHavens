---
title: "Showcasing Sampling Schemas with Sloths"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Showcasing Sampling Schemas with Sloths}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

`safeHavens` can be installed directly from github, no plans are made to release the package to CRAN as it's user base is relatively niche relative to a typical R package. 

```{r Install Package, eval  = F}
# install.packages('devtools') # `devtools`, or a similar package, is required to install packages from github
# devtools::install_github('sagesteppe/safeHavens')
# install.packages('remotes')  # lot's of user have issues with devtools, on Windows I believe, but `remotes` tends to work well. 
remotes::install_github('sagesteppe/safeHavens') # blah even same function name!
```

Once installed `safeHavens` can be loaded like any other package, whether they are installed from CRAN or github. 
We will also load `ggplot2` for it's excellent ability to plot simple features, and `tidyverse` for pushing some columns around etc.

```{r Load Libraries}
library(safeHavens)
library(ggplot2)
library(sf)
library(terra)
suppressPackageStartupMessages(library(dplyr))
```

# Notes about `safeHavens`

This package helps germplasm curators communicate areas of interest to collection teams for them to collect new material for accession.
It provides six different sampling approaches for curators to choose from for each individual taxon they hope to process.
It also allows for easy integration into existing workflows and will put out the required spatial data to share with collection teams.

Each of the approaches are based on standard practices in ecology, and reflect very basic tenets of population genetics. 
They have various trade offs in terms of computational and environmental complexity. 
The table below presents the currently implemented sampling schema and the user facing function associated with them. 
In my mind the first four functions are really flavors of the same process, one whereby we try and partition the species range into chunks of similar sizes. 
However, as is often the case four things which seem very similar to me may have enormously different results in implementation. 
The fifth method `IBDBasedSample` is largely in a class of it's own, in lieu of using the *continuity* of geographic space as it's primary method, it focuses on the discontinuity of space and using distance matrices and clustering to determine which patches of range are more close to each than other patches. 
The impetus behind this method is of course Sewall Wrights version of Isolation by Distance (1943). 

The `EcoregionBasedSample` may be the most commonly encountered method in North America and in various formats is driving two major germplasm banking projects in both the Midwest and Southeastern United States, as well as at a high level, composing the way that numerous native seed collection coordinators are structured in the West. 
This method using environmental variation as an implicit guide to targeting populations for seed collections, i.e. the different ecoregions serve as a stratification agent. 
In broad strokes, the general thinking is that these regions represent continuous transitions in the environment faced by the species, and populations across these ranges will be differently adapted to these environments. 
Given it's relative popularity in implementation, this function has more arguments than it's counterparts, which will be discussed below. 

The final function `EnvironmentalBasedSample` is both the most computationally expensive, and the most environmentally explicit. 
This function will rely on a Species Distribution Model, generated via a generalized linear model, supported by this package, to cluster populations based on environmental variables related to their observed distributions and the spatial configuration and distance between them. 
On paper, this draws together all aspects of the above functions, however no testing of this approach has been implemented. 
It will be discussed below in depth. 


|        Function           |              Description               | Comp.| Envi.|
|---------------------------|----------------------------------------|------|------|
| `GridBasedSample`         | Creates and merges *n* grids over area |  L   |   L  |
| `PointBasedSample`        | Creates points to make pieces over area|  L   |   L  |
| `EqualAreaSample`         | Breaks area into similar size pieces   |  L   |   L  |
| `OpportunisticSample`     | Using PBS with existing records        |  L   |   L  |
| `IBDBasedSample`          | Breaks species range into clusters     |  H   |   M  |
| `EcoregionBasedSample`    | Using existing ecoregions to sample    |  L   |   H  |
| `EnvironmentalBasedSample`| Uses correlations from SDM to sample   |  H   |   H  |
Note in this table 'Comp.' and 'Envi.' refer to computational and environmental complexity respectively, and range from low (L) through medium to high. 

# General notes about this vignette

This package is strongly focused on plants, no disrespect to animals, they just never occurred to me during development, and I'm sure you all have enough money things like this don't matter anyways. 
Otherwise just ask a FAANG do donate a team of developers - I'm sure they would be happy to oblige and blow this rag tag thing out of the water. 
However, to make it up to animal people, we will use a Sloth, which I think is (*Bradypus variegatus*)[https://en.wikipedia.org/wiki/Brown-throated_sloth] it's both a maniacally and mischievous looking little thing. 
Beyond looking at the inspiration for most Jim Henson puppets above, we will treat Bradypus more like a plant species when we discuss these sampling schemes - that is we will assume we are focused on organisms which seldom move great distances in dispersal processes.

We can access some data on *Bradypus* from the `dismo` package - presumably short for '*Dis*tribution *Mo*delling', which was created by Robert Hijman and others. 
We will use a couple other `dismo` functions in this package, and we will utilize some spatial data from it, It's a good package to be familiar with. If you want to read more about dismo and distribution modelling in general here an awesome bookdown resource written by (Hijman and Elith)[https://rspatial.org/raster/sdm/] - aspects of this vignette are definitely based on it. 
We will use Species Distribution Models as the input variable to many of the arguments in this function - don't get caught up in the details of making them. 
These days you can get good results out of essentially entirely automated pipelines, we'll have the truncated process outlined here in a chunk, but I honestly argue you should ignore this chunk the first few times you read this document, and go back for the details (discussed in the link above) only if you decide to pursue the route of `EnvironmentalBasedSample` 

# Species Distribution Modelling (skip me your first few read throughs?!)

If you are interested in visualizing what a sampling schema under safeHavens most complex function `EnvironmentalBasedSample` requires, you can explore this section to understand the costs required to achieve this process. 
It goes on for a while, so I almost recommend checking this section last. 
In the meantime, if you still want to see the results from `EnvironmentalBasedSample` compared to the other functions, you can load the results from disk, they are distributed with the package. 

```{r Prepare data for a Species Distribution Model}

x <- read.csv(file.path(system.file(package="dismo"), 'ex', 'bradypus.csv'))
x <- x[,c('lon', 'lat')]
x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)

files <- list.files(
  path = file.path(system.file(package="dismo"), 'ex'), 
  pattern = 'grd',  full.names=TRUE )
predictors <- terra::rast(files) # import the independent variables

```

The goal of most SDM's is to create a model which most accurately predicts where a species will be located in environmental, and hence geographic space. 
The goal of these models are rather to understand the degree to which various environmental features correlate with a species observed range. 
Accordingly, they are not optimized for use in making the typical decisions associated with species distribution models. 
Rather they are focused on estimating the effects of various parameters on the species distribution. 

```{r Create Species Distribution Model, message=F, warning=F, eval = F}

sdModel <- elasticSDM(
  x = x, predictors = predictors, quantile_v = 0.025,
  planar_proj =
    '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs')

```

We use the `caret` package to help out with our glmnet modelling, it's unnecessary, but it provides output which is very easy to explore and interact with. 
What makes an elastic net model interesting is that it is able to bridge the worlds of lasso and ridge regression by blending the alpha parameter. 
Lasso has an alpha of 0, while Ridge has an alpha of 1. 
Lasso regression will perform automated variable selection, and can actually drop (or 'shrink') them from a model, while Ridge regression will keep all variables and if correlated features are present give some contributions to each of them. 
An elastic net blends this propensity to drop or retain variables whenever it is used. 
Caret will test a range of alphas to accomplish this. 

Note that by using this we don't exactly get a feel for which variable is really correlated with a an event, but we do at least get a sense of how a set of variables affects the range. 
Infer from this at your own risk!
Inference from SDM's is something I do not recommend except in the most special of circumstances anyways. 

```{r Explore SDM output - Different alpha, eval = F}
sdModel$CVStructure
```

Here we can see how the elastic net decided which is the top model. 
We used Accuracy rather than Kappa as the main criterion for model selection. 

```{r Explore SDM output - Confusion Matrix, eval = F}
sdModel$ConfusionMatrix
```

Here we can see how our selected model works on predicting the state of test data.
Note these accuracy results are slightly higher than those from the CV folds. 
This is not a bug, CV folds are testing on their own holdouts, while this is a brand new set of holdouts. 
The main reason the confustion matrix results are likely to be higher is due to spatial auto-correlation which are not address in a typical 'random split' of test and train data. 
Consider the ouput from CVStructure to be a little bit more realistic. 

```{r Explore SDM output - Map, eval = F}
terra::plot(sdModel$RasterPredictions)
```

SDM's produce surfaces which display the probability of suitable habitat across a landscape. 
Many people want a binary prediction surface from them, i.e. is it more or less probable that suitable habitat for this taxon exists in this particular location (raster cell). 
Going from a continuous probability surface to a binary surface loses a lot of information, but in many use cases is essential to reduce computational complexity. 
We will use binary surfaces for implementing our sampling procedures across a species range. 
The function `PostProcessSDM` is used for this purpose. 

Historically, when assessing probability output 0.5 probability was used as a threshold, probabilities beneath if being considered 'Not suitable', while probabilities above became 'Suitable'. 
This works OK for some certain use cases, but we argue that thresholding is outside the domain of statistics and in the realm of practice. 
My motto for implementing models is that "*All models are wrong, some are useful - how do you want to be wrong?*". 
Our goal of sampling for germplasm conservation is to maximize the representation of allelic diversity across the range of a species. 
In order to do this, we need a good understanding of what the species actual range is, hence I am more happy to predict the species is present where it is not, than to predict it is absent where it is. 
Hence my preferred thresholding statistic is Sensitivity, over any metric which weighs false predicted presences. 
This argument is free to vary and supports any of the threshold values calculated by `dismo::threshold`, explore it to better understand it's options. 

Until now, you may be wondering why the function which achieves this is named `PostProcessSDM` rather `ThresholdSDM`, the reason for this perceived discontinuity is that the function does another process in it's second portion. 
Using all initial occurrence data, both the sets that went into our training and test data for developing the statistical model, we create 'buffers' around these points to ensure that none of the known occurrence points are 'missing' from the output binary map. 
This is a two edged sword, where we are again address the notion of dispersal limitation, and realize that not all suitable habitat is occupied habitat. 

What the `PostProcessSDM` function does is it again creates Cross Validation folds, and selects all of our training data. 
In each fold if then calculates the distance from each occurrence point to it's nearest neighbor. 
We then summarize these distances and can understand the distribution of distances as a quantile. 
We use a selected quantile, to then serve as a buffer. 
Area with predicted suitable habitat outside of this buffer become 'cut off' (or masked) from the binary raster map, and areas within buffer distance from known occurrences which are currently masked are reassigned as probabilities. 
The theory behind this process in underdeveloped and nascent, it does come down to the gut of an analyst. 
With the Bradypus data set I use 0.25 as the quantile, which is saying "Neighbors are generally 100km apart, I am happy with the risk of saying that 25km within each occurrence is occupied suitable habitat".
Increasing this value to say 1.0 will mean that no suitable habitat is removed, decreasing it makes maps more conservative. 
The cost with increasing the distances greatly is that the sampling methods may then puts grids in many areas without populations to collect from. 

```{r Threshold the SDM output, eval = F}

threshold_rasts <- PostProcessSDM(
  rast_cont = sdModel$RasterPredictions, 
  test = sdModel$TestData,
  planar_proj =
    '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs',
  thresh_metric = 'sensitivity', quant_amt = 0.5)

```

We can compare the results of applying this function side by side using the output from the function. 
```{r Compare Threshold Results, eval = F}
terra::plot(threshold_rasts$FinalRasters)
```

glmnet is used for three main reasons, 1) it gives directional coefficients and so we have a feel for how each 1 unit increase in an independent variable predicts the response, In my mind this is a big improvement over 'Variable Importance Factors' where we just know that certain variables contributed more to the model than others. 
2) It maintains some degree of automated selection reducing the work an analyst needs to do, i.e. you can process many species without spending too much time on any single one. 
And 3) glmnet actually re-scales all variables before model generation, which I suppose can be implemented with other models, we will use the same re-scaling that glmnet does to transform our independent variables in the raster stack and then multiply them by their beta-coefficients. 
In this way our raster stack becomes representative of our model, we can then use these values as the basis for hierarchical cluster later on. 

```{r Rescale Predictor Variables, eval = F}
########### CREATE A COPY OF THE RASTER PREDICTORS WHERE WE HAVE 
# STANDARDIZED EACH VARIABLE - SO IT IS EQUIVALENT TO THE INPUT TO THE GLMNET
# FUNCTION, AND THEN MULTIPLIED IT BY IT'S BETA COEFFICIENT FROM THE FIT MODEL
# we will also write out the beta coefficients using writeSDMresults right after
# this. 
rr <- RescaleRasters(
  model = sdModel$Model,
  predictors = sdModel$Predictors, 
  training_data = sdModel$TrainData, 
  pred_mat = sdModel$PredictMatrix)

terra::plot(rr$RescaledPredictors)
```

We can see that the variables are 'close' to be on the same scale, this will work in a clustering algorithm. 
If any of the layers are all the same color (maybe yellow?) that means they have no variance, that's a term that was shrunk from the model. 
It will be dealt with internall in a future function. 

```{r Show beta Coefficients from model, eval = F}
print(rr$BetaCoefficients)
```
We can also look at the beta coefficients for each variable. 
glmnet returns the 'untransformed' variables, i.e. the coefficients on the same scale as the input rasters, we calculate the BC right afterwards. 

`safeHavens` generates all kinds of things as it runs through the functions `elasticSDM`, `PostProcessSDM`, and `RescaleRasters`. 
Given then one of these sampling schema may be followed for quite some time, I think it is best practice to save many of these objects. 
Yes they will take up some storage space, but storage is virtually free these days anyways. 
So why not write out all of the following items? 
I test write them in a directory which exists for the project associated with the creation of this R package, just save them somewhere and delete them after this. 
These test files are tiny anyways. 

```{r Save SDM results, eval = F}

bp <- '~/Documents/assoRted/StrategizingGermplasmCollections'

writeSDMresults(
  cv_model = sdModel$CVStructure, 
  pcnm = sdModel$PCNM, 
  model = sdModel$Model, 
  cm = sdModel$ConfusionMatrix, 
  coef_tab = rr$BetaCoefficients, 
  f_rasts = threshold_rasts$FinalRasters,
  thresh = threshold_rasts$Threshold,
  file.path(bp, 'results', 'SDM'), 'Bradypus_test')

# we can see that the fils were placed here using this. 
list.files( file.path(bp, 'results', 'SDM'), recursive = TRUE )
```

And there you have it, all the steps to make a species distribution model - or rather to get the coefficients from a species distribution model!
We will play around with it as an example dataset to compare our buffered distance results to. 

```{r Clean up SDM variables}
rm(rr, predictors, files, sdModel, bp)
```

# Alternative to SDM, just buffer!

So the SDM is a whole thing, and not all users may want to create them. 
Here we provide an approach which will give us results we can work with, and is computationally cheap as dirt. 
As a minimalist that would probably just use the `EqualAreaBasedSample` function, this would almost certainly be the route I took to gridding out areas. 

It relies on one input, the same occurrence data as above, and a simple process - drawing a circle at a specific radius around each point!

```{r Just Buffer it!}
planar_proj =
    '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs'

x <- read.csv(file.path(system.file(package="dismo"), 'ex', 'bradypus.csv'))
x <- x[,c('lon', 'lat')]
x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)
x_buff <- sf::st_transform(x, planar_proj) |>
  sf::st_buffer(125000) |> # we are working in planar metric coordinates, we are
  sf::st_as_sfc() |> # buffer by this many / 1000 kilometers. 
  sf::st_union()

plot(x_buff)
rm(x)
```

Voila! That's the whole process. Just play around with the distances until you get something which looks OK. 
How do I define 'OK' a feeling in that gut that your boots are tingling. 

# Just load the SDM!
 
The package also has an SDM projection saved in the data we can just load that too. 
```{r}
sdm <- terra::rast(file.path(system.file(package="safeHavens"), 'data/Bradypus_test.tif'))
terra::plot(sdm)
```

# Running the Various Sample Design Algorithms

Now that we have some data which can represent species ranges, we can run the various sampling approaches. 
The table in the introduction is reproduced here for your leisure. 

|        Function           |              Description               | Comp.| Envi.|
|---------------------------|----------------------------------------|------|------|
| `GridBasedSample`         | Creates and merges *n* grids over area |  L   |   L  |
| `PointBasedSample`        | Creates points to make pieces over area|  L   |   L  |
| `EqualAreaSample`         | Breaks area into similar size pieces   |  L   |   L  |
| `OpportunisticSample`     | Using PBS with existing records        |  L   |   L  |
| `IBDBasedSample`          | Breaks species range into clusters     |  H   |   M  |
| `EcoregionBasedSample`    | Using existing ecoregions to sample    |  L   |   H  |
| `EnvironmentalBasedSample`| Uses correlations from SDM to sample   |  H   |   H  |
Note in this table 'Comp.' and 'Envi.' refer to computational and environmental complexity respectively, and range from low (L) through medium to high. 

### Grid Based and Point Based Sample

Ecologists love grids. 
All of us are taught to love grids for sampling. 
Grids are create for contiguous things. 
Species ranges are often not like this. 
Surprisingly coding the functions to create a grid based sample where the most difficult of anything in this whole package. 
So why did I do it, you ask. 
Because I love grids, I also was already loathe to hear the question after a talk "Why didn't you code grids? I love grids? I don't think these comparisons are complete until we see grids. We gave you *hors d'oeuvres* and you gave us shit..."

You can look at the output below to see why grids are not great for this type of problem. 

The first step in grid sampling is determining an OK number of grids to try and draw as a starting point, if we want 20 collections we will need more than 20 grids, because several will be merged into the larger ones. 
Using the aspect ratio of a simple bounding box around the area will be analysing, the function will determine a default number of grids ('Original') for testing. 
Using these defaults it will create a few other sets of grids as well, by either removing one of two grids per direction. 
Theoretically you could automate grid selection by comparing the number of grids and the minimization of variance. 
To be safe I wouldn't consider configurations which generate less than 25 of these initial grids. 

In the table below I op for using the 'Smaller' option, with 28 grids generated by prompting `sf::st_make_grid` with 7 grids in the x direction and 5 in the y direction. 

```{r Test Grid Sizes}
tgs <- TestGridSizes(x_buff)
print(tgs)
tgs <- tgs[tgs$Name=='Smaller',]
```

```{r Grid and Point Based Samples}

grid_buff <- GridBasedSample(x_buff, planar_proj, gridDimensions = tgs) 

ggplot(data = grid_buff) + 
  geom_sf(aes(fill = factor(Assigned))) + 
  geom_sf_label(aes(label = Assigned))

vector_sdm <- terra::as.polygons(sdm$Supplemented) |>
  sf::st_as_sf() |>
  sf::st_make_valid()

sf::st_crs(vector_sdm) == sf::st_crs(planar_proj)

grid_buff <- GridBasedSample(vector_sdm, planar_proj, gridDimensions = tgs) 
```


```{r Point Based Sample}

pbs <- PointBasedSample(x_buff)
pbs.sf <- pbs$Geometry

ggplot(data = pbs.sf) + 
  geom_sf(aes(fill = factor(ID))) + 
  geom_sf_label(aes(label = ID))

```


### Equal Area Sample

Perhaps the simplest method which is offered in `safeHavens` is `EqualAreaSample`. 
It simply creates many points, `pts` defaults to 5000, within our target polygon and then subjects them to k-means sampling where the groups are specified by `n` our target number of collections. 
The individual points then assigned to these groups have polygons which 'take' up all of the map space developed, and are intersected back to the species range, and the area of each polygon is then measured. 
This process will be ran a few times, defaulting to 100 `reps`, and the set of polygons which was created during these reps with the smallest variance in polygon size will be selected to be returned from the function. 

```{r Equal Area Sample}
eas <- EqualAreaSample(x_buff, planar_projection = planar_proj) 
eas.sf <- eas$Geometry

ggplot(data = eas.sf) + 
  geom_sf(aes(fill = factor(ID))) + 
  geom_sf_label(aes(label = ID))
```


### Opportunistic Sample

To be blunt, while there are many new players to the germplasm conservation table (and we are thrilled to have you here!), many existing collections have largely grown out of opportunity (which we are still very happy about). 
Many Curators may be interested in how much they can embed their existing collections into a sampling framework. 
The function `OpportunisticSample` makes a few very minor modifications to the point based sample to try and maximize an existing collection. 
It doesn't always work exceptionally, especially when a couple collections are very close to each other, but it may be a beneficial tool in the belt. 

Essentially 
```{r Opportunistic Sample}

exist_pts <- sf::st_sample(x_buff, size = 10) |>
   sf::st_as_sf() |>
   dplyr::rename(geometry = x)

os <- OpportunisticSample(polygon = x_buff, n = 20, collections = exist_pts)
os.sf <- os$Geometry

ggplot(data = os.sf) + 
  geom_sf(aes(fill = factor(ID))) + 
  geom_sf_label(aes(label = ID)) + 
  geom_sf(data = exist_pts)
```


### Isolation by Distance Based Sample


```{r Isolate by Distance Example}
files <- list.files( # note that for this process we need a raster rather than 
  path = file.path(system.file(package="dismo"), 'ex'), # vector data to accomplish
  pattern = 'grd',  full.names=TRUE ) # this we will 'rasterize' the vector using terra
predictors <- terra::rast(files) # this can also be done using 'fasterize'. Whenever
# we rasterize a product, we will need to provide a template raster that our vector
# will inherit the cell size, coordinate system, etc. from 

x_buff.sf <- sf::st_as_sf(x_buff) |> 
  dplyr::mutate(Range = 1) |> 
  sf::st_transform( terra::crs(predictors))

# and here we specify the field/column with our variable we want to become an attribute of our raster
v <- terra::rasterize(x_buff.sf, predictors, field = 'Range') 

# now we run the function demanding 20 areas to make accessions from, 
ibdbs <- IBDBasedSample(x = v, n = 20, fixedClusters = TRUE, template = predictors)

ggplot(data = ibdbs) +
  geom_sf(aes(fill = factor(Cluster))) + 
  geom_sf_label(aes(label = Cluster))

rm(predictors, files, v, x_buff.sf)
```

### Ecoregion Based Sample 

As mentioned this is by far the most commonly implemented method for guiding native seed collection. 
However, I am not sure exactly how practitioners all implement it, and whether the formats of application are consistent among practitioners!
For these reasons a few different sets of options are supported for a user. 

For general usage, two parameters are always required `x` which is the species range as an sf object, and `ecoregions`, the sf object containing the ecoregions of interest.
The `ecoregions` file does not need to be subset to the range of `x` quite yet - the function will take care of that. 
Additional arguments to the function include as usual `n` to specify how many accession we are looking for in our collection. 
Two additional arguments relate to whether we are using Omernik Level 4 ecoregions data or ecoregions (or biogeographic regions) from another source. 
These are `OmernikEPA`, and `ecoregion_col`, if you are using the official EPA release of ecoregions then both of these are optional, however if you are not using the EPA product than both *should* be supplied - but only the `ecoregion_col` argument is totally necessary. 
This column should contain unique names for the highest resolution level ecoregion you want to use from the data set, for many data sets, such as our example we call 'neo_eco' this may be the only field with ecolevel information!

```{r Ecoregion Based Sample}
neo_eco <- sf::st_read(
  system.file("data/NeoTropicsEcoregions.gpkg", package="safeHavens"), 
  quiet = TRUE)
head(neo_eco[,c(1, 3, 4, 6, 11)])

EcoregionBasedSample(sp_range, neo_eco, OmernikEPA = FALSE, ecoregion_col = 'Provincias')
```


### Environmental Based Sample 

The environmental based sample can only be conducted if you have the species distribution model data. 
```{r Environmental Based Sample}

setwd('~/Documents/assoRted/StrategizingGermplasmCollections')
getwd()

wm <- EnvironmentalBasedSample(
  pred_rescale = rr$RescaledPredictors, 
  path = file.path('results', 'SDM'),
  taxon = 'Bradypus_test', 
  f_rasts = f_rasts, n = 20, fixedClusters = TRUE, n_pts = 500, 
  planar_proj = 
    '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs',
  buffer_d = 3, prop_split = 0.8)

```

# Comparision of different sampling schema

So, we've made got some maps for you to look at!
They all look relatively similar to me when plotted one after another, let's plot them all simultaneously and see if that's still the case. 

```{r Plot all Sampling Schemas together}

```


