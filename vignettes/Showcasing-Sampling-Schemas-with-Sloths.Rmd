---
title: "Showcasing Sampling Schemas with Sloths"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Showcasing Sampling Schemas with Sloths}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, eval  = F}
library(safeHavens)
```


# Notes about `safeHavens`

This package helps germplasm curators communicate areas of interest to collection teams for them to collect new material for accession.
It provides six different sampling approaches for curators to choose from for each individual taxon they hope to process.
It also allows for easy integration into existing workflows and will put out the required spatial data to share with collection teams.

Each of the approaches are based on standard practices in ecology, and reflect very basic tenets of population genetics. 
They have various trade offs in terms of computational and environmental complexity. 
The table below presents the currently implemented sampling schemas and the user facing function associated with them. 
In my mind the first four functions are really flavors of the same process, one whereby we try and partition the species range into chunks of similar sizes. 
However, as is often the case four things which seem very similar to me may have enormously different results in implementation. 
The fifth method `IBDBasedSample` is largely in a class of it's own, in lieu of using the *continuity* of geographic space as it's primary method, it focuses on the discontinuity of space and using distance matrices and clustering to determine which patches of range are more close to each than other patches. 
The impetus behind this method is of course Sewall Wrights version of Isolation by Distance (1943). 

The `EcoregionBasedSample` may be the most commonly encountered method in North America and in various formats is driving two major germplasm banking projects in both the Midwest and Southeastern United States, as well as at a high level, composing the way that numerous native seed collection coordinators are structured in the West. 
This method using environmental variation as an implicit guide to targeting populations for seed collections, i.e. the different ecoregions serve as a stratification agent. 
In broad strokes, the general thinking is that these regions represent continuous transitions in the environment faced by the species, and populations across these ranges will be differently adapted to these environments. 
Given it's relative popularity in implementation, this function has more arguments than it's counterparts, which will be discussed below. 

The final function `EnvironmentalBasedSample` is both the most computationally expensive, and the most environmentally explicit. 
This function will rely on a Species Distribution Model, generated via a generalized linear model, supported by this package, to cluster populations based on environmental variables related to their observed distributions and the spatial configuration and distance between them. 
On paper, this draws together all aspects of the above functions, however no testing of this approach has been implemented. 
It will be discussed below in depth. 


|        Function           |              Description               | Comp.| Envi.|
|---------------------------|----------------------------------------|------|------|
| `GridBasedSample`         | Creates and merges *n* grids over area |  L   |   L  |
| `PointBasedSample`        | Creates points to make pieces over area|  L   |   L  |
| `EqualAreaSample`         | Breaks area into similar size pieces   |  L   |   L  |
| `OpportunisticSample`     | Using PBS with existing records        |  L   |   L  |
| `IBDBasedSample`          | Breaks species range into clusters     |  H   |   M  |
| `EcoregionBasedSample`    | Using existing ecoregions to sample    |  L   |   H  |
| `EnvironmentalBasedSample`| Uses correlations from SDM to sample   |  H   |   H  |
Note in this table 'Comp.' and 'Envi.' refer to computational and environmental complexity respectively, and range from low (L) through medium to high. 

# General notes about this vignette

This package is strongly focused on plants, no disrespect to animals, they just never occurred to me during development, and I'm sure you all have enough money things like this don't matter anyways. 
Otherwise just ask a FAANG do donate a team of developers - I'm sure they would be happy to oblige and blow this rag tag thing out of the water. 
However, to make it up to animal people, we will use a Sloth, which I think is (*Bradypus variegatus*)[https://en.wikipedia.org/wiki/Brown-throated_sloth] it's both a maniacally and mischievous looking little thing. 
Beyond looking at the inspiration for most Jim Henson puppets above, we will treat Bradypus more like a plant species when we discuss these sampling schemes - that is we will assume we are focused on organisms which seldom move great distances in dispersal processes.

We can access some data on *Bradypus* from the `dismo` package - presumably short for '*Dis*tribution *Mo*delling', which was created by Robert Hijman and others. 
We will use a couple other `dismo` functions in this package, and we will utilize some spatial data from it, It's a good package to be familiar with. If you want to read more about dismo and distribution modelling in general here an awesome bookdown resource written by (Hijman and Elith)[https://rspatial.org/raster/sdm/] - aspects of this vignette are definitely based on it. 
We will use Species Distribution Models as the input variable to many of the arguments in this function - don't get caught up in the details of making them. 
These days you can get good results out of essentially entirely automated pipelines, we'll have the truncated process outlined here in a chunk, but I honestly argue you should ignore this chunk the first few times you read this document, and go back for the details (discussed in the link above) only if you decide to pursue the route of `EnvironmentalBasedSample` 

# Species Distribution Modelling (skip me your first few read throughs?!)

```{r Prepare data for a Species Distribution Model, eval = F}

x <- read.csv(file.path(system.file(package="dismo"), 'ex', 'bradypus.csv'))
x <- x[,c('lon', 'lat')]
x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)

files <- list.files(
  path = file.path(system.file(package="dismo"), 'ex'), 
  pattern = 'grd',  full.names=TRUE )
predictors <- terra::rast(files) # import the independent variables

```

The goal of most SDM's is to create a model which most accurately predicts where a species will be located in environmental, and hence geographic space. 
The goal of these models are rather to understand the degree to which various environmental features correlate with a species observed range. 
Accordingly, they are not optimized for use in making the typical decisions associated with species distribution models. 
Rather they are focused on estimating the effects of various parameters on the species distribution. 

```{r Create Species Distribution Model}

sdModel <- elasticSDM(
  x = x, predictors = predictors, quantile_v = 0.025,
  planar_proj =
    '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs')

```

We use the `caret` package to help out with our glmnet modelling, it's unnecessary, but it provides output which is very easy to explore and interact with. 
What makes an elastic net model interesting is that it is able to bridge the worlds of lasso and ridge regression by blending the alpha parameter. 
Lasso has an alpha of 0, while Ridge has an alpha of 1. 
Lasso regression will perform automated variable selection, and can actually drop (or 'shrink') them from a model, while Ridge regression will keep all variables and if correlated features are present give some contributions to each of them. 
An elastic net blends this propensity to drop or retain variables whenever it is used. 
Caret will test a range of alphas to accomplish this. 

Note that by using this we don't exactly get a feel for which variable is really correlated with a an event, but we do at least get a sense of how a set of variables affects the range. 
Infer from this at your own risk!
Inference from SDM's is something I do not recommend except in the most special of circumstances anyways. 

```{r Explore SDM output - Different alpha}
sdModel$CVStructure
```

Here we can see how the elastic net decided which is the top model. 
We used Accuracy rather than Kappa as the main criterion for model selection. 

```{r Explore SDM output - Confusion Matrix}
sdModel$ConfusionMatrix
```

Here we can see how our selected model works on predicting the state of test data.
Note these accuracy results are slightly higher than those from the CV folds. 
This is not a bug, CV folds are testing on their own holdouts, while this is a brand new set of holdouts. 
The main reason the confustion matrix results are likely to be higher is due to spatial auto-correlation which are not address in a typical 'random split' of test and train data. 
Consider the ouput from CVStructure to be a little bit more realistic. 

```{r Explore SDM output - Map}
terra::plot(sdModel$RasterPredictions)
```

SDM's produce surfaces which display the probability of suitable habitat across a landscape. 
Many people want a binary prediction surface from them, i.e. is it more or less probable that suitable habitat for this taxon exists in this particular location (raster cell). 
Going from a continuous probability surface to a binary surface loses a lot of information, but in many use cases is essential to reduce computational complexity. 
We will use binary surfaces for implementing our sampling procedures across a species range. 
The function `PostProcessSDM` is used for this purpose. 

Historically, when assessing probability output 0.5 probability was used as a threshold, probabilities beneath if being considered 'Not suitable', while probabilities above became 'Suitable'. 
This works OK for some certain use cases, but we argue that thresholding is outside the domain of statistics and in the realm of practice. 
My motto for implementing models is that "*All models are wrong, some are useful - how do you want to be wrong?*". 
Our goal of sampling for germplasm conservation is to maximize the representation of allelic diversity across the range of a species. 
In order to do this, we need a good understanding of what the species actual range is, hence I am more happy to predict the species is present where it is not, than to predict it is absent where it is. 
Hence my preferred thresholding statistic is Sensitivity, over any metric which weighs false predicted presences. 
This argument is free to vary and supports any of the threshold values calculated by `dismo::threshold`, explore it to better understand it's options. 

Until now, you may be wondering why the function which achieves this is named `PostProcessSDM` rather `ThresholdSDM`, the reason for this perceived discontinuity is that the function does another process in it's second portion. 
Using all initial occurrence data, both the sets that went into our training and test data for developing the statistical model, we create 'buffers' around these points to ensure that none of the known occurrence points are 'missing' from the output binary map. 
This is a two edged sword, where we are again address the notion of dispersal limitation, and realize that not all suitable habitat is occupied habitat. 

What the `PostProcessSDM` function does is it again creates Cross Validation folds, and selects all of our training data. 
In each fold if then calculates the distance from each occurrence point to it's nearest neighbor. 
We then summarize these distances and can understand the distribution of distances as a quantile. 
We use a selected quantile, to then serve as a buffer. 
Area with predicted suitable habitat outside of this buffer become 'cut off' (or masked) from the binary raster map, and areas within buffer distance from known occurrences which are currently masked are reassigned as probabilities. 
The theory behind this process in underdeveloped and nascent, it does come down to the gut of an analyst. 
With the Bradypus data set I use 0.25 as the quantile, which is saying "Neighbors are generally 100km apart, I am happy with the risk of saying that 25km within each occurrence is occupied suitable habitat".
Increasing this value to say 1.0 will mean that no suitable habitat is removed, decreasing it makes maps more conservative. 
The cost with increasing the distances greatly is that the sampling methods may then puts grids in many areas without populations to collect from. 

```{r Threshold the SDM output}

threshold_rasts <- PostProcessSDM(
  rast_cont = sdModel$RasterPredictions, 
  test = sdModel$TestData,
  planar_proj =
    '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs',
  thresh_metric = 'sensitivity', quant_amt = 0.25)

```

We can compare the results of applying this function side by side using the output from the function. 
```{r Compare Threshold Results}


```

glmnet is used for three main reasons, 1) it gives directional coefficients and so we have a feel for how each 1 unit increase in an independent variable predicts the response, In my mind this is a big improvement over 'Variable Importance Factors' where we just know that certain variables contributed more to the model than others. 
2) It maintains some degree of automated selection reducing the work an analyst needs to do, i.e. you can process many species without spending too much time on any single one. 
And 3) glmnet actually re-scales all variables before model generation, which I suppose can be implemented with other models, we will use the same re-scaling that glmnet does to transform our independent variables in the raster stack and then multiply them by their beta-coefficients. 
In this way our raster stack becomes representative of our model, we can then use these values as the basis for hierarchical cluster later on. 

```{r Rescale Predictor Variables}
########### CREATE A COPY OF THE RASTER PREDICTORS WHERE WE HAVE 
# STANDARDIZED EACH VARIABLE - SO IT IS EQUIVALENT TO THE INPUT TO THE GLMNET
# FUNCTION, AND THEN MULTIPLIED IT BY IT'S BETA COEFFICIENT FROM THE FIT MODEL
# we will also write out the beta coefficients using writeSDMresults right after
# this. 
rr <- RescaleRasters(
  model = sdModel$Model,
  predictors = sdModel$Predictors, 
  training_data = sdModel$TrainData, 
  pred_mat = sdModel$PredictMatrix)

terra::plot(rr$RescaledPredictors)
print(rr$BetaCoefficients)
```


`safeHavens` generates all kinds of things as it runs through the functions `elasticSDM`, `PostProcessSDM`, and `RescaleRasters`. 
Given then one of these sampling schema may be followed for quite some time, I think it is best practice to save many of these objects. 
Yes they will take up some storage space, but storage is virtually free these days anyways. 
So why not write out all of the following items? 
I test write them in a directory which exists for the project associated with the creation of this R package, just save them somewhere and delete them after this. 
These test files are tiny anyways. 

```{r Save SDM results}

setwd('~/Documents/assoRted/StrategizingGermplasmCollections')
getwd() # verify we are working from where we think we are. 

writeSDMresults(
  cv_model = sdModel$CVStructure, 
  pcnm = sdModel$PCNM, 
  model = sdModel$Model, 
  cm = sdModel$ConfusionMatrix, 
  coef_tab = rr$BetaCoefficients, 
  f_rasts = threshold_rasts$FinalRasters,
  thresh = threshold_rasts$Threshold,
  file.path('results', 'SDM'), 'Bradypus_test')

```

And there you have it, all the steps to make a species distribution model - or rather to get the coefficients from a species distribution model!

# Alternative to SDM, just buffer!

So the SDM is a whole thing, and not all users may want to create them. 
Here we provide an approach which will give us results we can work with, and is computationally cheap as dirt. 
As a minimalist that would probably just use the `EqualAreaBasedSample` function, this would almost certainly be the route I took to gridding out areas. 

It relies on one input, the same occurrence data as above, and a simple process - drawing a circle at a specific radius around each point!

```{r Just Buffer it!}
planar_proj =
    '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs'

x <- read.csv(file.path(system.file(package="dismo"), 'ex', 'bradypus.csv'))
x <- x[,c('lon', 'lat')]
x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)
x_buff <- sf::st_transform(x, planar_proj) |>
  sf::st_buffer(125000) |> # we are working in planar metric coordinates, we are
  sf::st_as_sfc() |> # buffer by this many / 1000 kilometers. 
  sf::st_union()

plot(x_buff)
rm(x)
```


Voila! That's the whole process. Just play around with the distances until you get something which looks OK. 
How do I define 'OK' a feeling in that gut that your boots are tingling. 

# Running the Various Sample Design Algorithms

Now that we have some data which can represent species ranges, we can run the various sampling approaches. 

### Grid Based and Point Based Sample

Ecologists love grids. 
All of us are taught to love grids for sampling. 
Grids are create for contiguous things. 
Species ranges are often not like this. 
Surprisingly coding the functions to create a grid based sample where the most difficult of anything in this whole package. 
So why did I do it, you ask. 
Because I love grids, I also was already loathe to hear the question after a talk "Why didn't you code grids? I love grids? I don't think these comparisons are complete until we see grids. We gave you bagels and you gave us shit..."

You can look at the output below to see why grids are not great for this type of problem. 

```{r Grid and Point Based Samples}
library(ggplot2)
grzzz <- GridBasedSample(x_buff, planar_proj) 

ggplot(data = grzzz) + 
  geom_sf(aes(fill = factor(Assigned))) + 
  geom_sf_label(aes(label = Assigned))

###########3 PBS NEEDS TO ASSIGN POINTS ON THE EXPORT OF DATA!!!!!!!!!!!!!!!!!!!
#######################
################333333
#########################
pbs <- PointBasedSample(grzzz)

pbs.sf <- pbs$Geometry

ggplot(data = pbs.sf) + 
  geom_sf()# + 
  geom_sf_label(aes(label = Assigned))

```


### Equal Area Sample

Perhaps the simplest method which is offered in `safeHavens` is `EqualAreaSample`. 
It simply creates many points, `pts` defaults to 5000, within our target polygon and then subjects them to k-means sampling where the groups are specified by `n` our target number of collections. 
The individual points then assigned to these groups have polygons which 'take' up all of the map space developed, and are intersected back to the species range, and the area of each polygon is then measured. 
This process will be ran a few times, defaulting to 100 `reps`, and the set of polygons which was created during these reps with the smallest variance in polygon size will be selected to be returned from the function. 

```{r Equal Area Sample}
eas <- EqualAreaSample(x_buff, planar_projection = planar_proj) 
eas.sf <- eas$Geometry

ggplot(data = eas.sf) + 
  geom_sf(aes(fill = factor(Cluster))) + 
  geom_sf_label(aes(label = Cluster))
```


### Opportunistic Sample

```{r Opportunistic Sample}

exist_pts <- sf::st_sample(x_buff, size = 5) |>
   sf::st_as_sf() |>
   dplyr::rename(geometry = x)

os <- OpportunisticSample(polygon = x_buff, n = 20, collections = exist_pts)
os.sf <- os$Geometry

ggplot(data = os.sf) + 
  geom_sf(aes(fill = factor(ID))) + 
  geom_sf_label(aes(label = ID)) + 
  geom_sf(data = exist_pts)
```


### Isolation by Distance Based Sample


```{r Isolate by Distance }
files <- list.files(
  path = file.path(system.file(package="dismo"), 'ex'), 
  pattern = 'grd',  full.names=TRUE )
predictors <- terra::rast(files)

x_buff.sf <- sf::st_as_sf(x_buff) |> 
  dplyr::mutate(Range = 1) |> 
  sf::st_transform( terra::crs(predictors))

v <- terra::rasterize(x_buff.sf, predictors, field = 'Range')
ibdbs <- IBDBasedSample(x = v, n = 20, fixedClusters = TRUE, template = predictors)

ggplot(data = ibdbs) +
  geom_sf(aes(fill = factor(Cluster))) + 
  geom_sf_label(aes(label = Cluster))

rm(predictors, files, v, x_buff.sf)
```

### Ecoregion Based Sample 

```{r}

```


### Environmental Based Sample 


```{r Environmental Based Sample}

wm <- EnvironmentalBasedSample(
  pred_rescale = rr$RescaledPredictors, 
  path = file.path('results', 'SDM'),
  taxon = 'Bradypus_test', 
  f_rasts = f_rasts, n = 20, fixedClusters = TRUE, n_pts = 500, 
  planar_proj = 
    '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs',
  buffer_d = 3, prop_split = 0.8)

```

# Comparision of different sampling schema